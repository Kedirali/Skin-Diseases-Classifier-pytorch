{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XeCGJp0auo_O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random import seed\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import models\n",
    "import pretrainedmodels as ptm\n",
    "import sklearn.metrics as skmet\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as nnF\n",
    "import pathlib\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ta0sGtGxYO_",
    "outputId": "7bbbf5c9-0183-479e-88e5-1fe5c7414069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will run on cpu\n"
     ]
    }
   ],
   "source": [
    "# But first we need to tell PyTorch where to 'keep' the model \n",
    "# On GPU or on CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print('The model will run on', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "PCCRudF-xYQv"
   },
   "outputs": [],
   "source": [
    "#Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=50),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((50,50)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "q75crykIxYTw"
   },
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path='E:/N-Dataset/BINARY/Final/train'\n",
    "valid_path = 'E:/N-Dataset/BINARY/Final/validation'\n",
    "test_path='E:/N-Dataset/BINARY/Final/test'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=train_transform),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(valid_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "UlKbtc0_xYVU"
   },
   "outputs": [],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "3EiSnzVvxYYp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abnormal', 'Healthy']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "lvv-ikxqxYaM"
   },
   "outputs": [],
   "source": [
    "# Load pretrained ResNet50 Model\n",
    "model = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "QBzgwQ18xYdZ"
   },
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameter\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs=model.classifier[-1].in_features\n",
    "model.classifier=nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "    nn.Linear(in_features=num_ftrs, out_features=2, bias=True))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features.0.0.weight False\n",
      "1 features.0.1.weight False\n",
      "2 features.0.1.bias False\n",
      "3 features.1.conv.0.0.weight False\n",
      "4 features.1.conv.0.1.weight False\n",
      "5 features.1.conv.0.1.bias False\n",
      "6 features.1.conv.1.weight False\n",
      "7 features.1.conv.2.weight False\n",
      "8 features.1.conv.2.bias False\n",
      "9 features.2.conv.0.0.weight False\n",
      "10 features.2.conv.0.1.weight False\n",
      "11 features.2.conv.0.1.bias False\n",
      "12 features.2.conv.1.0.weight False\n",
      "13 features.2.conv.1.1.weight False\n",
      "14 features.2.conv.1.1.bias False\n",
      "15 features.2.conv.2.weight False\n",
      "16 features.2.conv.3.weight False\n",
      "17 features.2.conv.3.bias False\n",
      "18 features.3.conv.0.0.weight False\n",
      "19 features.3.conv.0.1.weight False\n",
      "20 features.3.conv.0.1.bias False\n",
      "21 features.3.conv.1.0.weight False\n",
      "22 features.3.conv.1.1.weight False\n",
      "23 features.3.conv.1.1.bias False\n",
      "24 features.3.conv.2.weight False\n",
      "25 features.3.conv.3.weight False\n",
      "26 features.3.conv.3.bias False\n",
      "27 features.4.conv.0.0.weight False\n",
      "28 features.4.conv.0.1.weight False\n",
      "29 features.4.conv.0.1.bias False\n",
      "30 features.4.conv.1.0.weight False\n",
      "31 features.4.conv.1.1.weight False\n",
      "32 features.4.conv.1.1.bias False\n",
      "33 features.4.conv.2.weight False\n",
      "34 features.4.conv.3.weight False\n",
      "35 features.4.conv.3.bias False\n",
      "36 features.5.conv.0.0.weight False\n",
      "37 features.5.conv.0.1.weight False\n",
      "38 features.5.conv.0.1.bias False\n",
      "39 features.5.conv.1.0.weight False\n",
      "40 features.5.conv.1.1.weight False\n",
      "41 features.5.conv.1.1.bias False\n",
      "42 features.5.conv.2.weight False\n",
      "43 features.5.conv.3.weight False\n",
      "44 features.5.conv.3.bias False\n",
      "45 features.6.conv.0.0.weight False\n",
      "46 features.6.conv.0.1.weight False\n",
      "47 features.6.conv.0.1.bias False\n",
      "48 features.6.conv.1.0.weight False\n",
      "49 features.6.conv.1.1.weight False\n",
      "50 features.6.conv.1.1.bias False\n",
      "51 features.6.conv.2.weight False\n",
      "52 features.6.conv.3.weight False\n",
      "53 features.6.conv.3.bias False\n",
      "54 features.7.conv.0.0.weight False\n",
      "55 features.7.conv.0.1.weight False\n",
      "56 features.7.conv.0.1.bias False\n",
      "57 features.7.conv.1.0.weight False\n",
      "58 features.7.conv.1.1.weight False\n",
      "59 features.7.conv.1.1.bias False\n",
      "60 features.7.conv.2.weight False\n",
      "61 features.7.conv.3.weight False\n",
      "62 features.7.conv.3.bias False\n",
      "63 features.8.conv.0.0.weight False\n",
      "64 features.8.conv.0.1.weight False\n",
      "65 features.8.conv.0.1.bias False\n",
      "66 features.8.conv.1.0.weight False\n",
      "67 features.8.conv.1.1.weight False\n",
      "68 features.8.conv.1.1.bias False\n",
      "69 features.8.conv.2.weight False\n",
      "70 features.8.conv.3.weight False\n",
      "71 features.8.conv.3.bias False\n",
      "72 features.9.conv.0.0.weight False\n",
      "73 features.9.conv.0.1.weight False\n",
      "74 features.9.conv.0.1.bias False\n",
      "75 features.9.conv.1.0.weight False\n",
      "76 features.9.conv.1.1.weight False\n",
      "77 features.9.conv.1.1.bias False\n",
      "78 features.9.conv.2.weight False\n",
      "79 features.9.conv.3.weight False\n",
      "80 features.9.conv.3.bias False\n",
      "81 features.10.conv.0.0.weight False\n",
      "82 features.10.conv.0.1.weight False\n",
      "83 features.10.conv.0.1.bias False\n",
      "84 features.10.conv.1.0.weight False\n",
      "85 features.10.conv.1.1.weight False\n",
      "86 features.10.conv.1.1.bias False\n",
      "87 features.10.conv.2.weight False\n",
      "88 features.10.conv.3.weight False\n",
      "89 features.10.conv.3.bias False\n",
      "90 features.11.conv.0.0.weight False\n",
      "91 features.11.conv.0.1.weight False\n",
      "92 features.11.conv.0.1.bias False\n",
      "93 features.11.conv.1.0.weight False\n",
      "94 features.11.conv.1.1.weight False\n",
      "95 features.11.conv.1.1.bias False\n",
      "96 features.11.conv.2.weight False\n",
      "97 features.11.conv.3.weight False\n",
      "98 features.11.conv.3.bias False\n",
      "99 features.12.conv.0.0.weight False\n",
      "100 features.12.conv.0.1.weight False\n",
      "101 features.12.conv.0.1.bias False\n",
      "102 features.12.conv.1.0.weight False\n",
      "103 features.12.conv.1.1.weight False\n",
      "104 features.12.conv.1.1.bias False\n",
      "105 features.12.conv.2.weight False\n",
      "106 features.12.conv.3.weight False\n",
      "107 features.12.conv.3.bias False\n",
      "108 features.13.conv.0.0.weight False\n",
      "109 features.13.conv.0.1.weight False\n",
      "110 features.13.conv.0.1.bias False\n",
      "111 features.13.conv.1.0.weight False\n",
      "112 features.13.conv.1.1.weight False\n",
      "113 features.13.conv.1.1.bias False\n",
      "114 features.13.conv.2.weight False\n",
      "115 features.13.conv.3.weight False\n",
      "116 features.13.conv.3.bias False\n",
      "117 features.14.conv.0.0.weight False\n",
      "118 features.14.conv.0.1.weight False\n",
      "119 features.14.conv.0.1.bias False\n",
      "120 features.14.conv.1.0.weight False\n",
      "121 features.14.conv.1.1.weight False\n",
      "122 features.14.conv.1.1.bias False\n",
      "123 features.14.conv.2.weight False\n",
      "124 features.14.conv.3.weight False\n",
      "125 features.14.conv.3.bias False\n",
      "126 features.15.conv.0.0.weight False\n",
      "127 features.15.conv.0.1.weight False\n",
      "128 features.15.conv.0.1.bias False\n",
      "129 features.15.conv.1.0.weight False\n",
      "130 features.15.conv.1.1.weight False\n",
      "131 features.15.conv.1.1.bias False\n",
      "132 features.15.conv.2.weight False\n",
      "133 features.15.conv.3.weight False\n",
      "134 features.15.conv.3.bias False\n",
      "135 features.16.conv.0.0.weight False\n",
      "136 features.16.conv.0.1.weight False\n",
      "137 features.16.conv.0.1.bias False\n",
      "138 features.16.conv.1.0.weight False\n",
      "139 features.16.conv.1.1.weight False\n",
      "140 features.16.conv.1.1.bias False\n",
      "141 features.16.conv.2.weight False\n",
      "142 features.16.conv.3.weight False\n",
      "143 features.16.conv.3.bias False\n",
      "144 features.17.conv.0.0.weight False\n",
      "145 features.17.conv.0.1.weight False\n",
      "146 features.17.conv.0.1.bias False\n",
      "147 features.17.conv.1.0.weight False\n",
      "148 features.17.conv.1.1.weight False\n",
      "149 features.17.conv.1.1.bias False\n",
      "150 features.17.conv.2.weight False\n",
      "151 features.17.conv.3.weight False\n",
      "152 features.17.conv.3.bias False\n",
      "153 features.18.0.weight False\n",
      "154 features.18.1.weight False\n",
      "155 features.18.1.bias False\n",
      "156 classifier.1.weight True\n",
      "157 classifier.1.bias True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-9c9d0388f008>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "for num, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(num, name, param.requires_grad )\n",
    "summary(model, input_size=(3, 224, 224))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Xpq2xu_GxYjM"
   },
   "outputs": [],
   "source": [
    "class AVGMetrics (object):\n",
    "    \"\"\"\n",
    "        This is a simple class to control the AVG for a given value. It's used to control loss and accuracy for start\n",
    "        and evaluate partition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sum_value = 0\n",
    "        self.avg = 0\n",
    "        self.count = 0\n",
    "        self.values = []\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.avg\n",
    "\n",
    "    def update(self, val):\n",
    "        self.values.append(val)\n",
    "        self.sum_value += val\n",
    "        self.count += 1\n",
    "        self.avg = self.sum_value / float(self.count)\n",
    "\n",
    "    def print (self):\n",
    "        print('\\nsum_value: ', self.sum_value)\n",
    "        print('count: ', self.count)\n",
    "        print('avg: ', self.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "DomWQFomxsPD"
   },
   "outputs": [],
   "source": [
    "class TrainHistory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        \n",
    "    \n",
    "    def update (self, loss_train, loss_val, acc_train, acc_val):\n",
    "\n",
    "        self.train_loss.append(loss_train)\n",
    "        self.val_loss.append(loss_val)\n",
    "        self.train_acc.append(acc_train)\n",
    "        self.val_acc.append(acc_val)\n",
    "\n",
    "\n",
    "    def save (self, folder_path):\n",
    "\n",
    "        path = os.path.join(folder_path, 'history')\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        print (\"Saving history CSVs in {}\".format(path))\n",
    "\n",
    "        np.savetxt(os.path.join(path, \"train_loss.csv\"), np.asarray(self.train_loss), fmt='%.3f', delimiter=',')\n",
    "        np.savetxt(os.path.join(path, \"val_loss.csv\"), np.asarray(self.val_loss), fmt='%.3f', delimiter=',')\n",
    "\n",
    "        np.savetxt(os.path.join(path, \"train_acc.csv\"), np.asarray(self.train_acc), fmt='%.3f', delimiter=',')\n",
    "        np.savetxt(os.path.join(path, \"val_acc.csv\"), np.asarray(self.val_acc), fmt='%.3f', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "    def save_plot (self, folder_path):\n",
    "        path = os.path.join(folder_path, 'history')\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        epochs = [i + 1 for i in range(len(self.train_loss))]\n",
    "\n",
    "        print(\"Saving history plots in {}\".format(path))\n",
    "\n",
    "        plt.plot(epochs, self.train_loss, color='r', linestyle='solid')\n",
    "        plt.plot(epochs, self.val_loss, color='b', linestyle='solid')\n",
    "        plt.grid(color='black', linestyle='dotted', linewidth=0.7)\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path, \"loss_history.jpg\"), dpi=300)\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(epochs, self.train_acc, color='r', linestyle='solid')\n",
    "        plt.plot(epochs, self.val_acc, color='b', linestyle='solid')\n",
    "        plt.grid(color='black', linestyle='dotted', linewidth=0.7)\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path, \"acc_history.jpg\"), dpi=200)\n",
    "\n",
    "        plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "Wmxol7dgxscM"
   },
   "outputs": [],
   "source": [
    "def save_model (model, folder_path, epoch, opt_fn, loss_fn, is_best, multi_gpu=False, verbose=False):\n",
    "\n",
    "    last_check_path = os.path.join(folder_path, 'last-checkpoint')\n",
    "    best_check_path = os.path.join(folder_path, 'best-checkpoint')\n",
    "\n",
    "    if not os.path.exists(last_check_path):\n",
    "        if verbose:\n",
    "            print ('last-checkpoint folder does not exist. I am creating it!')\n",
    "        os.mkdir(last_check_path)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print ('last-checkpoint folder exist! Perfect, I will just use it.')\n",
    "\n",
    "    if not os.path.exists(best_check_path):\n",
    "        if verbose:\n",
    "            print('best-checkpoint folder does not exist. I am creating it!')\n",
    "        os.mkdir(best_check_path)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('best-checkpoint folder exist! Perfect, I will just use it.')\n",
    "\n",
    "    info_to_save = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict() if multi_gpu else model.state_dict(),\n",
    "        'optimizer_state_dict': opt_fn.state_dict(),\n",
    "        'loss': loss_fn,\n",
    "    }\n",
    "\n",
    "    torch.save(info_to_save, os.path.join(last_check_path, \"last-checkpoint.pth\"))\n",
    "\n",
    "    if is_best:\n",
    "        torch.save(info_to_save, os.path.join(best_check_path, 'MCC.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "6DrvcGUfxssU"
   },
   "outputs": [],
   "source": [
    "def load_model (checkpoint_path, model, opt_fn=None, loss_fn=None, epoch=None):\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise Exception (\"The {} does not exist!\".format(checkpoint_path))\n",
    "\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "    if opt_fn is not None and loss_fn is not None:\n",
    "        opt_fn.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        epoch = ckpt['epoch']\n",
    "        loss_fn = ckpt['loss']\n",
    "        return model, opt_fn, loss_fn, epoch\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_dim (lab_real, lab_pred, mode='labels'):\n",
    "    if mode == 'labels':\n",
    "        if lab_real.ndim == 2:\n",
    "            lab_real = lab_real.argmax(axis=1)\n",
    "        if lab_pred.ndim == 2:\n",
    "            lab_pred = lab_pred.argmax(axis=1)\n",
    "\n",
    "    elif mode == 'scores':\n",
    "        if lab_real.ndim == 1:\n",
    "            lab_real = one_hot_encoding(lab_real)\n",
    "        if lab_pred.ndim == 1:\n",
    "            lab_pred = one_hot_encoding(lab_pred)\n",
    "\n",
    "    else:\n",
    "        raise Exception ('There is no mode called {}. Please, choose between score or labels'.format(mode))\n",
    "\n",
    "    return lab_real, lab_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (lab_real, lab_pred, verbose=False):\n",
    "    # Checkin the array dimension\n",
    "    lab_real, lab_pred = _check_dim (lab_real, lab_pred, mode='labels')\n",
    "\n",
    "    acc = skmet.accuracy_score(lab_real, lab_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print('- Accuracy - {:.3f}'.format(acc))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix (lab_real, lab_pred, normalize=True):\n",
    "\n",
    "    # Checkin the array dimension\n",
    "    lab_real, lab_pred = _check_dim(lab_real, lab_pred, mode='labels')\n",
    "\n",
    "    cm = skmet.confusion_matrix(lab_real, lab_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(cm, class_names, normalize=True, save_path=None, title='Confusion matrix', cmap=plt.cm.GnBu):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=0)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if isinstance(save_path, str):\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.clf()\n",
    "    elif save_path:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainHistory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        \n",
    "    \n",
    "    def update (self, loss_train, loss_val, acc_train, acc_val):\n",
    "        \"\"\"\n",
    "        This function appends a new value to the loss/train loss and acc. These values are stored by epoch\n",
    "        :param loss_train: the train loss of the ith epoch\n",
    "        :param loss_val: the val loss of the ith epoch\n",
    "        :param acc_train: the train accuracy of the ith epoch\n",
    "        :param acc_val: the val accuracy of the ith epoch\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_loss.append(loss_train)\n",
    "        self.val_loss.append(loss_val)\n",
    "        self.train_acc.append(acc_train)\n",
    "        self.val_acc.append(acc_val)\n",
    "\n",
    "\n",
    "    def save (self, folder_path):\n",
    "        \"\"\"\n",
    "        This function saves the loss and accuracy history as csv files\n",
    "        :param folder_path: a string with the base folder path\n",
    "        \"\"\"\n",
    "\n",
    "        path = os.path.join(folder_path, 'history')\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        print (\"Saving history CSVs in {}\".format(path))\n",
    "\n",
    "        np.savetxt(os.path.join(path, \"train_loss.csv\"), np.asarray(self.train_loss), fmt='%.3f', delimiter=',')\n",
    "        np.savetxt(os.path.join(path, \"val_loss.csv\"), np.asarray(self.val_loss), fmt='%.3f', delimiter=',')\n",
    "\n",
    "        np.savetxt(os.path.join(path, \"train_acc.csv\"), np.asarray(self.train_acc), fmt='%.3f', delimiter=',')\n",
    "        np.savetxt(os.path.join(path, \"val_acc.csv\"), np.asarray(self.val_acc), fmt='%.3f', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "    def save_plot (self, folder_path):\n",
    "        \"\"\"\n",
    "        This function saves a plot of the loss and accuracy history\n",
    "        :param folder_path: a string with the base folder path\n",
    "        \"\"\"\n",
    "\n",
    "        path = os.path.join(folder_path, 'history')\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        epochs = [i + 1 for i in range(len(self.train_loss))]\n",
    "\n",
    "        print(\"Saving history plots in {}\".format(path))\n",
    "\n",
    "        plt.plot(epochs, self.train_loss, color='r', linestyle='solid')\n",
    "        plt.plot(epochs, self.val_loss, color='b', linestyle='solid')\n",
    "        plt.grid(color='black', linestyle='dotted', linewidth=0.7)\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path, \"loss_history.jpg\"), dpi=300)\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(epochs, self.train_acc, color='r', linestyle='solid')\n",
    "        plt.plot(epochs, self.val_acc, color='b', linestyle='solid')\n",
    "        plt.grid(color='black', linestyle='dotted', linewidth=0.7)\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path, \"acc_history.jpg\"), dpi=200)\n",
    "\n",
    "        plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_report (lab_real, lab_pred, class_names=None, verbose=False, output_dict=False):\n",
    "\n",
    "    # Checking the array dimension\n",
    "    lab_real, lab_pred = _check_dim(lab_real, lab_pred, mode='labels')\n",
    "\n",
    "    report = skmet.classification_report(lab_real, lab_pred, target_names=class_names, output_dict=output_dict)\n",
    "\n",
    "    if verbose:\n",
    "         print(report)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "_ZtMQF6Qx8rJ"
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\n",
    "    def __init__(self, metrics_names, class_names=None, options=None):\n",
    "\n",
    "        self.metrics_names = metrics_names\n",
    "        self.metrics_values = dict()        \n",
    "        self.options = options\n",
    "        \n",
    "        self.pred_scores = None\n",
    "        self.label_scores = None\n",
    "        self.img_names = None\n",
    "        \n",
    "        self.class_names = class_names\n",
    "\n",
    "\n",
    "    def compute_metrics (self):\n",
    "\n",
    "        save_all_path = None\n",
    "        # Checking if save_all is informed\n",
    "        if self.options is not None:\n",
    "            if \"save_all_path\" in self.options.keys():\n",
    "                # Checking if the folder doesn't exist. If True, we must create it.\n",
    "                if not os.path.isdir(self.options[\"save_all_path\"]):\n",
    "                    os.mkdir(self.options[\"save_all_path\"])\n",
    "                save_all_path = self.options[\"save_all_path\"]\n",
    "\n",
    "        if self.metrics_names is None:\n",
    "            return None\n",
    "        \n",
    "        if self.metrics_names == \"all\":\n",
    "            self.metrics_names = [\"accuracy\", \"conf_matrix\", \"plot_conf_matrix\", \"precision_recall_report\"]\n",
    "        \n",
    "        \n",
    "        for mets in self.metrics_names:\n",
    "            if mets == \"accuracy\":\n",
    "                self.metrics_values[\"accuracy\"] = accuracy(self.label_scores, self.pred_scores)\n",
    "\n",
    "            \n",
    "            elif mets == \"conf_matrix\":\n",
    "                \n",
    "                # Checking the options\n",
    "                normalize = True\n",
    "                if self.options is not None:\n",
    "                    if \"normalize_conf_matrix\" in self.options.keys():\n",
    "                        normalize = self.options[\"normalize_conf_matrix\"]\n",
    "                \n",
    "                self.metrics_values[\"conf_matrix\"] = conf_matrix(self.label_scores, self.pred_scores, normalize)\n",
    "            elif mets == \"plot_conf_matrix\":\n",
    "                \n",
    "                # Checking if the class names are defined\n",
    "                if self.class_names is None:\n",
    "                    raise Exception (\"You are trying to plot the confusion matrix without defining the classes name\")\n",
    "                \n",
    "                # Checking the options\n",
    "                save_path = None\n",
    "                normalize = True\n",
    "                title = \"Confusion Matrix\"   \n",
    "                \n",
    "                if self.options is not None:\n",
    "                    if save_all_path is not None:\n",
    "                        save_path = os.path.join(save_all_path, \"conf_mat.jpg\")\n",
    "                    if \"save_path_conf_matrix\" in self.options.keys():\n",
    "                        save_path = self.options[\"save_path_conf_matrix\"]\n",
    "                    if \"normalize_conf_matrix\" in self.options.keys():\n",
    "                        normalize = self.options[\"normalize_conf_matrix\"]\n",
    "                    if \"title_conf_matrix\" in self.options.keys():\n",
    "                        title = self.options[\"title_conf_matrix\"]\n",
    "                        \n",
    "                if \"conf_matrix\" in self.metrics_values.keys():\n",
    "                    cm = self.metrics_values[\"conf_matrix\"]\n",
    "                else:\n",
    "                    cm = conf_matrix(self.label_scores, self.pred_scores, normalize)\n",
    "                \n",
    "                plot_conf_matrix(cm, self.class_names, normalize, save_path, title)\n",
    "                \n",
    "                \n",
    "            elif mets == \"precision_recall_report\":\n",
    "\n",
    "                self.metrics_values[\"precision_recall_report\"] = precision_recall_report(self.label_scores,\n",
    "                                                                                              self.pred_scores,\n",
    "                                                                                              self.class_names)\n",
    "\n",
    "            \n",
    "    def print (self):\n",
    "        \"\"\"\n",
    "        This method just prints the metrics on the screen\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.metrics_names is None:\n",
    "            print (\"Since metrics name is None, there is no metric to print\")\n",
    "            \n",
    "        else:        \n",
    "            for met in self.metrics_values.keys():\n",
    "                if met == \"loss\":\n",
    "                    print ('- Loss: {:.3f}'.format(self.metrics_values[met]))\n",
    "                elif met == \"accuracy\":\n",
    "                    print ('- Accuracy: {:.3f}'.format(self.metrics_values[met]))\n",
    "                elif met == \"conf_matrix\":\n",
    "                    print('- Confusion Matrix: \\n{}'.format(self.metrics_values[met]))\n",
    "                elif met == \"precision_recall_report\":\n",
    "                    print('- Precision and Recall report: \\n{}'.format(self.metrics_values[met]))\n",
    "\n",
    "    def add_metric_value (self, value_name, value):\n",
    "        \"\"\"\n",
    "        Adding a new value from a external source into the metrics\n",
    "        :param value_name (string): the key for the dict\n",
    "        :param value: the value to be saved in the self.metrics_values\n",
    "        \"\"\"\n",
    "        self.metrics_values[value_name] = value\n",
    "\n",
    "\n",
    "    def update_scores (self, label_batch, pred_batch, img_name_batch=None):\n",
    "        \"\"\"\n",
    "        The evaluation is made using batchs. So, every batch we get just a piece of the prediction. This method\n",
    "        concatenate all prediction and labels in order to compute the metrics\n",
    "        :param pred (np.array): an array containing part of the predictions outputed by the model\n",
    "        :param label (np.array): an array contaning the true labels\n",
    "        \"\"\"\n",
    "\n",
    "        if self.label_scores is None and self.pred_scores is None:\n",
    "            self.label_scores = label_batch\n",
    "            self.pred_scores = pred_batch\n",
    "        else:\n",
    "            if pred_batch is not None:\n",
    "                self.pred_scores = np.concatenate((self.pred_scores, pred_batch))\n",
    "            if label_batch is not None:\n",
    "                self.label_scores = np.concatenate((self.label_scores, label_batch))\n",
    "\n",
    "    def save_metrics (self, folder_path, name=\"metrics.txt\"):\n",
    "        \"\"\"\n",
    "        This method saves the computed metrics\n",
    "        :param folder_path (string): the folder you'd like to save the metrics\n",
    "        :param name (string): the file name. Default is metrics.txt\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.metrics_names is None:\n",
    "            print (\"Since metrics name is None, there is no metric to save\")\n",
    "            \n",
    "        else:        \n",
    "            with open(os.path.join(folder_path, name), \"w\") as f:\n",
    "    \n",
    "                f.write(\"- METRICS REPORT -\\n\\n\")\n",
    "    \n",
    "                for met in self.metrics_values.keys():\n",
    "                    if met == \"loss\":\n",
    "                        f.write('- Loss: {:.3f}\\n'.format(self.metrics_values[met]))\n",
    "                    elif met == \"accuracy\":\n",
    "                        f.write('- Accuracy: {:.3f}\\n'.format(self.metrics_values[met]))\n",
    "                    elif met == \"conf_matrix\":\n",
    "                        f.write('- Confusion Matrix: \\n{}\\n'.format(self.metrics_values[met]))\n",
    "                    elif met == \"precision_recall_report\":\n",
    "                        f.write('- Precision and Recall report: \\n{}\\n'.format(self.metrics_values[met]))\n",
    "\n",
    "\n",
    "    def save_scores (self, folder_path=None, pred_name=\"predictions.csv\"):\n",
    "        \"\"\"\n",
    "        This method saves the concatenated scores in the disk\n",
    "        :param folder_path (string): the folder you'd like to save the scores\n",
    "        :param pred_name (string): the predictions' score file name. Default is predictions.csv\n",
    "        :param labels_name (string): the labels' score file name. Default is labels.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if folder_path is not None:\n",
    "            # Checking if the folder doesn't exist. If True, we must create it.\n",
    "            if not os.path.isdir(folder_path):\n",
    "                os.mkdir(folder_path)\n",
    "        elif self.options is not None:\n",
    "            if \"save_all_path\" in self.options.keys():\n",
    "                folder_path = self.options[\"save_all_path\"]\n",
    "            elif \"save_path_scores\" in self.options.keys():\n",
    "                folder_path = self.options[\"save_path_scores\"]\n",
    "            else:\n",
    "                raise (\"The options doesnt have any folder to save the scores\")\n",
    "\n",
    "            if 'pred_name_scores' in self.options.keys():\n",
    "                pred_name = self.options['pred_name_scores']\n",
    "        else:\n",
    "            raise (\"You must set the path to save the score eithe in options or in folder_path parameter\")\n",
    "\n",
    "\n",
    "        # Getting the list of classications and predict labels\n",
    "        if self.class_names is not None:\n",
    "            if self.label_scores is not None:\n",
    "                real_labels = [self.class_names[int(l)] for l in self.label_scores]\n",
    "                real_labels = np.asarray(real_labels)\n",
    "                real_labels = real_labels.reshape(real_labels.shape[0], 1)\n",
    "\n",
    "            if self.img_names is not None:\n",
    "                img_names = np.asarray(self.img_names)\n",
    "                img_names = img_names.reshape(img_names.shape[0], 1)\n",
    "\n",
    "            pred_labels = [self.class_names[ps.argmax()] for ps in self.pred_scores]\n",
    "            pred_labels = np.asarray(pred_labels)\n",
    "            pred_labels = pred_labels.reshape(pred_labels.shape[0], 1)\n",
    "        else:\n",
    "            raise (\"You need to inform the class names to use this function\")\n",
    "\n",
    "        if self.img_names is not None and self.label_scores is not None:\n",
    "            both_data = np.concatenate((img_names, real_labels, pred_labels, self.pred_scores), axis=1)\n",
    "            cols = ['image', 'REAL', 'PRED', *self.class_names]\n",
    "        elif self.img_names is None and self.label_scores is not None:\n",
    "            both_data = np.concatenate((real_labels, pred_labels, self.pred_scores), axis=1)\n",
    "            cols = ['REAL', 'PRED', *self.class_names]\n",
    "        elif self.img_names is not None and self.label_scores is None:\n",
    "            both_data = np.concatenate((img_names, pred_labels, self.pred_scores), axis=1)\n",
    "            cols = ['image', 'PRED', *self.class_names]\n",
    "        else:\n",
    "            both_data = np.concatenate((real_labels, pred_labels, self.pred_scores), axis=1)\n",
    "            cols = ['REAL', 'PRED', *self.class_names]\n",
    "\n",
    "        df = pd.DataFrame(both_data, columns=cols)\n",
    "        print (\"Saving the scores in {}\".format(folder_path))\n",
    "\n",
    "        df.to_csv(os.path.join(folder_path, pred_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "9v59Iedjx8_5"
   },
   "outputs": [],
   "source": [
    "def _config_logger(save_path, file_name):\n",
    "    \"\"\"\n",
    "        Internal function to configure the logger\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    logger = logging.getLogger(\"Train-Logger\")\n",
    "    # Checking if the folder logs doesn't exist. If True, we must create it.\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    logger_filename = os.path.join(save_path, f\"{file_name}_{str(time.time()).replace('.','')}.log\")\n",
    "    fhandler = logging.FileHandler(filename=logger_filename, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def _train_epoch (model, optimizer, loss_fn, data_loader, c_epoch, t_epoch, device=None):\n",
    "  \n",
    "    # setting the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    print (\"Training...\")\n",
    "    # Setting tqdm to show some information on the screen\n",
    "    with tqdm(total=len(train_loader), ascii=True, desc='Epoch {}/{}: '.format(c_epoch, t_epoch), ncols=100) as t:\n",
    "\n",
    "\n",
    "        # Variables to store the avg metrics\n",
    "        loss_avg = AVGMetrics()\n",
    "        acc_avg = AVGMetrics()\n",
    "\n",
    "        # Getting the data from the DataLoader generator\n",
    "        for i, (images,labels) in enumerate(train_loader):\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images=Variable(images.cuda())\n",
    "                labels=Variable(labels.cuda())\n",
    "                # Doing the forward pass\n",
    "            out = model(images)\n",
    "            # Computing loss function\n",
    "            loss = loss_fn(out, labels)\n",
    "            # Computing the accuracy\n",
    "            acc = accuracy(out, labels)\n",
    "\n",
    "            # Getting the avg metrics\n",
    "            loss_avg.update(loss.item())\n",
    "            acc_avg.update(acc.item())\n",
    "\n",
    "            # Zero the parameters gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Computing gradients and performing the update step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Updating tqdm\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    return {\"loss\": loss_avg(), \"accuracy\": acc_avg()}\n",
    "\n",
    "def fit_model (model, train_loader, valid_loader, optimizer=None, loss_fn=None, epochs=1,\n",
    "               epochs_early_stop=None, save_folder=None, initial_model=None, best_metric=\"loss\", device=None,\n",
    "               schedule_lr=None, config_bot=None, model_name=\"CNN\", resume_train=False, history_plot=True,\n",
    "               val_metrics=('accuracy'), metric_early_stop=None):\n",
    "\n",
    "    logger = _config_logger(save_folder, model_name)\n",
    "    logger.info(\"Starting the training phase\")\n",
    "\n",
    "    if epochs_early_stop is not None:\n",
    "        logger.info('Early stopping is set using the number of epochs without improvement')\n",
    "    if metric_early_stop is not None:\n",
    "        logger.info('Early stopping is set using the min/max metric as threshold')\n",
    "    if epochs_early_stop is None and metric_early_stop is None:\n",
    "        logger.info('No early stopping is set')\n",
    "\n",
    "    history = TrainHistory()\n",
    "\n",
    "    # Checking if we have a saved model. If we have, load it, otherwise, let's start the model from scratch\n",
    "    epoch_resume = 0\n",
    "    if initial_model is not None:\n",
    "        logger.info(\"Loading the saved model in {} folder\".format(initial_model))\n",
    "\n",
    "        if resume_train:\n",
    "            model, optimizer, loss_fn, epoch_resume = load_model(initial_model, model)\n",
    "            logger.info(\"Resuming the training from epoch {} ...\".format(epoch_resume))\n",
    "        else:\n",
    "            model = load_model(initial_model, model)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"The model will be trained from scratch\")\n",
    "\n",
    "        # Moving the model to the given device\n",
    "    model.to(device)\n",
    "\n",
    "    # Setting data to store the best mestric\n",
    "    logging.info(\"The best metric to get the best model will be {}\".format(best_metric))\n",
    "    if best_metric == 'loss':\n",
    "        best_metric_value = 1000\n",
    "    else:\n",
    "        best_metric_value = 0\n",
    "    best_flag = False\n",
    "\n",
    "    # setting a flag for the early stop\n",
    "    early_stop_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # writer is used to generate the summary files to be loaded at tensorboard\n",
    "    writer = SummaryWriter (os.path.join(save_folder, 'summaries'))\n",
    "\n",
    "    \n",
    "    # Let's iterate for `epoch` epochs or a tolerance.\n",
    "    # It always start from epoch resume. If it's set, it starts from the last epoch the training phase was stopped,\n",
    "    # otherwise, it starts from 0\n",
    "    epoch = epoch_resume\n",
    "    while epoch < epochs:\n",
    "\n",
    "        # Updating epoch\n",
    "        epoch += 1\n",
    "\n",
    "        # Training and getting the metrics for one epoch\n",
    "        train_metrics = _train_epoch(model, optimizer, loss_fn, train_loader, epochs, device)\n",
    "\n",
    "        # After each epoch, we evaluate the model for the training and validation data\n",
    "        val_metrics = metrics_for_eval (model, valid_loader, device, loss_fn)\n",
    "\n",
    "        # Checking the schedule if applicable\n",
    "        if isinstance(schedule_lr, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            schedule_lr.step(best_metric_value)\n",
    "        elif isinstance(schedule_lr, torch.optim.lr_scheduler.MultiStepLR):\n",
    "            schedule_lr.step(epoch)\n",
    "\n",
    "        # Getting the current LR\n",
    "        current_LR = None\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_LR = param_group['lr']\n",
    "\n",
    "\n",
    "        writer.add_scalars('Loss', {'val-loss': val_metrics['loss'],\n",
    "                                                 'start-loss': train_metrics['loss']},\n",
    "                                                 epoch)\n",
    "\n",
    "        writer.add_scalars('Accuracy', {'val-loss': val_metrics['accuracy'],\n",
    "                                    'start-loss': train_metrics['accuracy']},\n",
    "                                    epoch)\n",
    "\n",
    "        history.update(train_metrics['loss'], val_metrics['loss'], train_metrics['accuracy'], val_metrics['accuracy'])\n",
    "\n",
    "\n",
    "        # Getting the metrics for the training partition epoch\n",
    "        train_print = \"-- Loss: {:.3f}\\n-- Acc: {:.3f}\\n\".format(train_metrics[\"loss\"],train_metrics[\"accuracy\"])\n",
    "\n",
    "        # Getting the metrics for the validation partition in this epoch\n",
    "        val_print = \"-- Loss: {:.3f}\\n-- Acc: {:.3f}\\n\".format(val_metrics[\"loss\"],val_metrics[\"accuracy\"])\n",
    "\n",
    "        early_stop_count += 1\n",
    "        new_best_print = None\n",
    "        # Defining the best metric for validation\n",
    "        if best_metric == 'loss':\n",
    "            if val_metrics[best_metric] <= best_metric_value:\n",
    "                best_metric_value = val_metrics[best_metric]\n",
    "                new_best_print = '\\n-- New best {}: {:.3f}'.format(best_metric, best_metric_value)\n",
    "                best_flag = True\n",
    "                best_epoch = epoch\n",
    "                early_stop_count = 0\n",
    "        else:\n",
    "            if val_metrics[best_metric] >= best_metric_value:\n",
    "                best_metric_value = val_metrics[best_metric]\n",
    "                new_best_print = '\\-- New best {}: {:.3f}'.format(best_metric, best_metric_value)\n",
    "                best_flag = True\n",
    "                best_epoch = epoch\n",
    "                early_stop_count = 0\n",
    "\n",
    "        # Check if it's the best model in order to save it\n",
    "        if save_folder is not None:\n",
    "            save_model(model, save_folder, epoch, optimizer, loss_fn, best_flag)\n",
    "        best_flag = False\n",
    "\n",
    "        # Updating the logger\n",
    "        msg = \"Metrics for epoch {} out of {}\\n\".format(epoch, epochs)\n",
    "        msg += \"- Train\\n\"\n",
    "        msg += train_print + \"\\n\"\n",
    "        msg += \"\\n- Validation\\n\"\n",
    "        msg += val_print + \"\\n\"\n",
    "        msg += \"\\n- Training info\"\n",
    "        msg += \"\\n-- Early stopping counting: {} max to stop is {}\".format(early_stop_count, epochs_early_stop)\n",
    "        msg += \"\\n-- Current LR: {}\".format(current_LR)\n",
    "        if new_best_print is not None:\n",
    "            msg += new_best_print\n",
    "        msg += \"\\n-- Best {} so far: {:.3f} on epoch {}\\n\".format(best_metric, best_metric_value, best_epoch)\n",
    "\n",
    "        # Checking the early stop\n",
    "        if epochs_early_stop is not None:\n",
    "            if early_stop_count >= epochs_early_stop:\n",
    "                logger.info(msg)\n",
    "                logger.info(\"The early stop trigger was activated. The validation {} \" .format(best_metric) +\n",
    "                            \"{:.3f} did not improved for {} epochs.\".format(best_metric_value,\n",
    "                                                                            epochs_early_stop) +\n",
    "                            \"The training phase was stopped.\")\n",
    "\n",
    "                break\n",
    "\n",
    "        # Checking the early stop\n",
    "        if metric_early_stop is not None:\n",
    "            stop = False\n",
    "            if best_metric == 'loss':\n",
    "                if metric_early_stop >= best_metric_value:\n",
    "                    stop = True\n",
    "            else:\n",
    "                if metric_early_stop <= best_metric_value:\n",
    "                    stop = True\n",
    "\n",
    "            if stop:\n",
    "                logger.info(msg)\n",
    "                logger.info(\"The early stop trigger was activated. The validation {} \".format(best_metric) +\n",
    "                            \"{:.3f} achieved the defined threshold {:.3f}.\".format(best_metric_value,\n",
    "                                                                            metric_early_stop) +\n",
    "                            \"The training phase was stopped.\")\n",
    "                break\n",
    "\n",
    "        # Sending all message to the logger\n",
    "        logger.info(msg)\n",
    "\n",
    "\n",
    "    if history_plot:\n",
    "        history.save_plot(save_folder)\n",
    "\n",
    "    history.save(save_folder)\n",
    "    print('\\n')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "jmkbLmAiyK_Q"
   },
   "outputs": [],
   "source": [
    "def metrics_for_eval (model, valid_loader, device, loss_fn):\n",
    "   # setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    print (\"\\nEvaluating...\")\n",
    "    # Setting tqdm to show some information on the screen\n",
    "    \n",
    "    with tqdm(total=len(valid_loader), ascii=True, ncols=100) as t:\n",
    "\n",
    "        # Setting require_grad=False in order to dimiss the gradient computation in the graph\n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss_avg = AVGMetrics()\n",
    "            acc_avg = AVGMetrics()\n",
    "\n",
    "\n",
    "            for i, (images,labels) in enumerate(valid_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    images=Variable(images.cuda())\n",
    "                    labels=Variable(labels.cuda())\n",
    "                    # Doing the forward pass without using meta-data\n",
    "                pred_batch = model(images)\n",
    "\n",
    "                # Computing the loss\n",
    "                L = loss_fn(pred_batch, labels)\n",
    "                # Computing the accuracy\n",
    "                acc  = accuracy(pred_batch, labels)\n",
    "\n",
    "                loss_avg.update(L.item())\n",
    "                acc_avg.update(acc.item())\n",
    "                \n",
    "                labels_batch_np = labels.cpu().numpy()\n",
    "\n",
    "                if Metrics is not None:\n",
    "                    # Moving the data to CPU and converting it to numpy in order to compute the metrics\n",
    "                    pred_batch_np = nnF.softmax(pred_batch,dim=1).cpu().numpy()\n",
    "                    # updating the scores\n",
    "                    Metrics.update_scores(labels_batch_np, pred_batch_np)\n",
    "\n",
    "                # Updating tqdm\n",
    "                t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "                t.update()\n",
    "\n",
    "    return {\"loss\": loss_avg(), \"accuracy\": acc_avg()}\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "def test_model (model, data_loader, checkpoint_path=None, loss_fn=None, device=None, save_pred=False,\n",
    "                    partition_name='Test', metrics_to_comp=('all'), class_names=None, metrics_options=None,\n",
    "                    apply_softmax=True, verbose=True, full_path_pred=None):\n",
    "\n",
    "    # setting the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    def _get_predictions (model, image):        \n",
    "        with torch.no_grad():\n",
    "            pred_batch = model(image)\n",
    "        return pred_batch\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        model = load_model(checkpoint_path, model)\n",
    "\n",
    "    if device is None:\n",
    "        # Setting the device\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:\" + str(torch.cuda.current_device()))\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # Moving the model to the given device\n",
    "    model.to(device)\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Setting the metrics object\n",
    "    metrics = Metrics (metrics_to_comp, class_names, metrics_options)\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    # Setting tqdm to show some information on the screen\n",
    "    with tqdm(total=len(valid_loader), ascii=True, ncols=100) as t:\n",
    "\n",
    "        loss_avg = AVGMetrics()\n",
    "        acc_avg = AVGMetrics()\n",
    "\n",
    "        for i, (images,labels) in enumerate(valid_loader):\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images=Variable(images.cuda())\n",
    "                labels=Variable(labels.cuda())\n",
    "\n",
    "                # Doing the forward pass without using meta-data\n",
    "            pred_batch = _get_predictions(model, images)\n",
    "            # Computing the loss\n",
    "            L = loss_fn(pred_batch, labels)\n",
    "            acc  = accuracy(pred_batch, labels)\n",
    "            loss_avg.update(L.item())\n",
    "            acc_avg.update(acc.item())\n",
    "                \n",
    "            labels_batch_np = labels.cpu().numpy()\n",
    "            # Moving the data to CPU and converting it to numpy in order to compute the metrics\n",
    "            if apply_softmax:\n",
    "                pred_batch_np = nnF.softmax(pred_batch,dim=1).cpu().numpy()\n",
    "            # updating the scores\n",
    "            metrics.update_scores(labels_batch_np, pred_batch_np)\n",
    "\n",
    "            # Updating tqdm\n",
    "            if metrics.metrics_names is None:\n",
    "                t.set_postfix(loss='{:05.3f}'.format(0.0))\n",
    "                t.set_postfix(accuracy='{:05.3f}'.format(0.0))\n",
    "            else:\n",
    "                t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "                #t.set_postfix(accuracy='{:05.3f}'.format(acc_avg()))\n",
    "\n",
    "            t.update()\n",
    "\n",
    "    # Adding loss into the metric values\n",
    "    metrics.add_metric_value(\"loss\", loss_avg())\n",
    "\n",
    "    # Getting the metrics\n",
    "    metrics.compute_metrics()\n",
    "\n",
    "    if save_pred or metrics.metrics_names is None:\n",
    "        if full_path_pred is None:\n",
    "            metrics.save_scores()\n",
    "        else:\n",
    "            _spt = full_path_pred.split('/')\n",
    "            _folder = \"/\".join(_spt[0:-1])\n",
    "            _p = _spt[-1]\n",
    "            metrics.save_scores(folder_path=_folder, pred_name=_p)\n",
    "\n",
    "    if verbose:\n",
    "        print('- {} metrics:'.format(partition_name))\n",
    "        metrics.print()\n",
    "\n",
    "\n",
    "    return metrics.metrics_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "FWRT9yT9yLBI"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "best_metric = \"loss\"\n",
    "lr_init = 0.0001\n",
    "sched_factor = 0.1\n",
    "sched_min_lr = 0.00001\n",
    "sched_patience = 10\n",
    "early_stop = 15\n",
    "save_folder = \"E:/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "TPWtX8V9yLD5"
   },
   "outputs": [],
   "source": [
    "def main (lr_init, sched_factor, sched_min_lr, sched_patience, batch_size, epochs, early_stop, save_folder, best_metric,):\n",
    "\n",
    "    metric_options = {\n",
    "        'save_all_path': os.path.join(save_folder, \"best_metrics\"),\n",
    "        'pred_name_scores': 'predictions_best_test.csv',\n",
    "        'normalize_conf_matrix': True}\n",
    "    checkpoint_best = os.path.join(save_folder, 'best-checkpoint/MCC.pth')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    print(\"- Loading...\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_init)\n",
    "    scheduler_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=sched_factor, min_lr=sched_min_lr,\n",
    "                                                                    patience=sched_patience)\n",
    "    ####################################################################################################################\n",
    "\n",
    "    print(\"- Starting the training phase...\")\n",
    "    print(\"-\" * 50)\n",
    "    fit_model (model, train_loader, valid_loader, optimizer=optimizer, loss_fn=loss_fn, epochs=epochs,\n",
    "               epochs_early_stop=early_stop, save_folder=save_folder, initial_model=None, device=None, \n",
    "               schedule_lr=scheduler_lr, model_name=\"CNN\", resume_train=False, history_plot=True,\n",
    "               best_metric=best_metric)\n",
    "    # Testing the validation partition\n",
    "    print(\"- Evaluating the validation partition...\")\n",
    "    test_model (model, valid_loader, checkpoint_path=checkpoint_best, loss_fn=loss_fn, save_pred=True, \n",
    "                metrics_to_comp=['accuracy'], class_names=classes, metrics_options=metric_options, \n",
    "                apply_softmax=True, verbose=False)\n",
    "\n",
    "    metric_options = {\n",
    "        'save_all_path': os.path.join(save_folder, \"best_metrics\"),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "        'plot_conf_matrix': True,\n",
    "        'normalize_conf_matrix': True\n",
    "    }\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Testing the test partition\n",
    "    print(\"\\n- Evaluating the testing partition...\")\n",
    "    test_model(model, test_loader, checkpoint_path=None, metrics_to_comp='all', class_names=classes,\n",
    "                   metrics_options=metric_options, save_pred=True, verbose=False)\n",
    "    ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8WlrIi-yLFu",
    "outputId": "3c577f77-ef97-4b5d-aa07-169802fd8327",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Train-Logger:Starting the training phase\n",
      "INFO:Train-Logger:Early stopping is set using the number of epochs without improvement\n",
      "INFO:Train-Logger:The model will be trained from scratch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading...\n",
      "- Starting the training phase...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/None:   0%|                                                          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/None: 100%|#####################################| 16/16 [00:10<00:00,  1.50it/s, loss=0.499]\n",
      "  0%|                                                                         | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_scores() missing 1 required positional argument: 'pred_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-1ad26b6bdfc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m k = main( lr_init = lr_init , sched_factor = sched_factor, sched_min_lr = sched_min_lr, sched_patience = sched_patience ,\n\u001b[0;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m          best_metric = best_metric)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-221-3d76a3f8b074>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(lr_init, sched_factor, sched_min_lr, sched_patience, batch_size, epochs, early_stop, save_folder, best_metric)\u001b[0m\n\u001b[0;32m     20\u001b[0m                \u001b[0mepochs_early_stop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                \u001b[0mschedule_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"CNN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                best_metric=best_metric)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Testing the validation partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"- Evaluating the validation partition...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-0e74883ff6a5>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, train_loader, valid_loader, optimizer, loss_fn, epochs, epochs_early_stop, save_folder, initial_model, best_metric, device, schedule_lr, config_bot, model_name, resume_train, history_plot, val_metrics, metric_early_stop)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# After each epoch, we evaluate the model for the training and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mval_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics_for_eval\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# Checking the schedule if applicable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-d1a03b765093>\u001b[0m in \u001b[0;36mmetrics_for_eval\u001b[1;34m(model, valid_loader, device, loss_fn)\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mpred_batch_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnnF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[1;31m# updating the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     \u001b[0mMetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_batch_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_batch_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m# Updating tqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: update_scores() missing 1 required positional argument: 'pred_batch'"
     ]
    }
   ],
   "source": [
    "k = main( lr_init = lr_init , sched_factor = sched_factor, sched_min_lr = sched_min_lr, sched_patience = sched_patience ,\n",
    "         batch_size = batch_size, epochs = epochs, early_stop = early_stop, save_folder = save_folder, \n",
    "         best_metric = best_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Image_only.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
